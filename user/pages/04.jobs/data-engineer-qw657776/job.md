---
title: 'Data Engineer'
date: '2023-03-09T19:43:27.286187Z'
modified: '2023-05-18T17:09:09.122653Z'
jobid: 580598
currency: USD
address: ''
status: active
career_page_url: 'https://www.careers-page.com/gentis/job/QW657776'
hash: QW657776
organization: 137896
salary_min: ''
salary_max: ''
avg_sal: ''
is_published: true
is_remote: true
created_at: '2023-03-09T19:43:27.286187Z'
updated_at: '2023-05-18T17:09:09.122653Z'
owner: 22120
contract_details: full_time
headcount: 1
compensation: Negotiable
hired_at: 0
submitted_at: 100
interview_at: 18
custom_fields:
    jobid: 'MKT-17 - Data Engineer Pipeline'
    visaallowed: true
    managernotes: "<p><span data-redactor-style-cache=\"font-size: 18px;\" data-redactor-span=\"true\" style=\"font-size: 18px;\"><strong><u>Pipeline job for Data Engineers - high quality candidates should be moved to the shortlisted stage</u></strong></span></p>\n<ol><li><span data-redactor-style-cache=\"font-size: 18px;\" data-redactor-span=\"true\" style=\"font-size: 18px;\">Record your Teams video interview with the candidate or use the Willow interview link. It's suggested you do live interviews as much as possible.</span></li><li><span data-redactor-style-cache=\"font-size: 18px;\" data-redactor-span=\"true\" style=\"font-size: 18px;\">Move quality candidates to the Shortlisted stage, save to a folder, ping your team lead and Dave.</span></li></ol>\n<p><span data-redactor-style-cache=\"font-size: 16px;\" data-redactor-span=\"true\" style=\"font-size: 16px;\">﻿<strong>When screening candidates, consider asking technical questions to assess their experience with the above skills and their ability to apply them in a practical context. Additionally, consider challenge questions to get more detailed responses from candidates who may not provide sufficient detail in their initial responses.</strong></span></p>\n<p><strong><span style=\"font-size: 16px;\" data-redactor-style-cache=\"font-size: 16px;\" data-redactor-span=\"true\"><br></span></strong></p>\n<p><strong><span style=\"font-size: 16px;\" data-redactor-style-cache=\"font-size: 16px;\" data-redactor-span=\"true\">Notes:</span></strong></p>\n<p><strong><span style=\"font-size: 16px;\" data-redactor-style-cache=\"font-size: 16px;\" data-redactor-span=\"true\"><u>﻿You should expect much stronger communication skills out of Data Engineers in comparison to a Java or QA Engineer</u><br></span></strong></p>\n<p>They are experienced in designing and building data pipelines and systems that can handle large volumes of data. A Data Engineer&nbsp;has significant experience working with a variety of data tools and technologies, including SQL, Python, Spark, Azure Data Factory, Azure Data Lake, and Azure Databricks. They are comfortable working with both structured and unstructured data and have experience with data modeling and database design.</p>\n<p>A Data Engineer is responsible for designing, building, and maintaining the data infrastructure for their organization on Azure. They work closely with data scientists and analysts to ensure that data is accurate, reliable, and accessible. Alex is responsible for building and optimizing data pipelines, ensuring data quality and consistency, and managing data storage and retrieval. They also ensure that the organization's data is secure and compliant with relevant regulations.</p>\n<p><span style=\"font-family: inherit; font-size: 0.875rem;\" data-redactor-span=\"true\" data-redactor-style-cache=\"font-family: inherit; font-size: 0.875rem;\"><strong>Personal Traits:</strong></span><br></p>\n<ul><li>Analytical and data-driven, with a passion for solving complex data problems</li><li>Detail-oriented and able to work with large volumes of data</li><li>Strong problem-solving skills, with the ability to identify and troubleshoot issues in data pipelines and systems</li><li><strong><u>Excellent communication and collaboration skills, with the ability to work effectively with business stake holders and&nbsp;cross-functional teams</u></strong></li></ul>\n<p><strong><span data-redactor-style-cache=\"font-size: 16px;\" style=\"font-size: 16px;\" data-redactor-span=\"true\">The life-cycle of data:</span></strong></p>\n<ol><li><strong>Data Origination:</strong> Data typically originates from various sources such as databases, applications, sensors, and IoT devices.</li><li><strong>Data Extraction:</strong> Data is extracted from its origination sources and loaded into data pipelines using various tools such as Azure Data Factory, which provides a set of built-in connectors to various data sources.</li><li><strong>Data Transformation:</strong> In the data pipeline, data is transformed and processed using tools like Databricks or Spark to clean, filter, and aggregate the data. This process helps to prepare the data for further analysis.</li><li><strong>Data Storage: </strong>After the data is transformed, it is stored in data warehouses like Azure Synapse, which provides a highly scalable, secure, and flexible way to store and analyze large amounts of data.</li><li><strong>Data Analysis:</strong> Once the data is stored in the data warehouse, it can be analyzed using various BI tools such as Power BI or Tableau to gain insights and make data-driven decisions.</li><li><strong>End of Process:</strong> At the end of the process, the insights gained from the data analysis can be used to improve business performance, optimize operations, or make other strategic decisions</li></ol>\n<p><strong><span style=\"font-size: 16px;\" data-redactor-style-cache=\"font-size: 16px;\" data-redactor-span=\"true\">﻿Data Pipeline:</span></strong></p>\n<p>﻿A data pipeline is a series of steps that data goes through in order to be processed, transformed, and analyzed. Data pipelines typically involve a series of tools and technologies that work together to move data from its original source, through various processing steps, and finally to its destination where it can be analyzed and used for insights.</p>\n<ol><li>E-commerce Data Pipeline: This data pipeline involves processing data from multiple sources, such as customer orders, website clicks, and inventory data. The pipeline includes steps such as data extraction, cleaning, transformation, and loading into a data warehouse. This pipeline can be used to analyze customer behavior, track inventory levels, and optimize pricing strategies.</li><li>IoT Data Pipeline: This data pipeline involves processing data from sensors and other devices connected to the internet of things (IoT). The pipeline includes steps such as data ingestion, filtering, aggregation, and storage in a data lake. This pipeline can be used to analyze machine performance, detect anomalies, and predict maintenance needs.</li><li>Social Media Data Pipeline: This data pipeline involves processing data from social media platforms such as Twitter, Facebook, and Instagram. The pipeline includes steps such as data extraction, transformation, and loading into a data warehouse. This pipeline can be used to analyze social media trends, track customer sentiment, and optimize marketing strategies.</li></ol>\n<p><strong><span style=\"font-size: 16px;\" data-redactor-style-cache=\"font-size: 16px;\" data-redactor-span=\"true\">Azure Data Factory:</span></strong></p>\n<p>Azure Data Factory (ADF) is a cloud-based data integration service that allows businesses to create, schedule, and manage data pipelines that move and transform data from various sources to various destinations.</p>\n<ol><li>Data Migration: ADF can be used to move data from an on-premises data center to a cloud-based data warehouse, such as Azure Synapse Analytics. This can be useful for businesses that want to migrate their data to the cloud for increased scalability, security, and accessibility.</li><li>Data Integration: ADF can be used to integrate data from multiple sources, such as databases, file systems, and SaaS applications. For example, ADF can be used to extract data from a SQL Server database, transform it, and load it into a Salesforce CRM system.</li><li>Big Data Processing: ADF can be used to create data pipelines that process large amounts of data using tools such as Azure Databricks or HDInsight. This can be useful for businesses that need to analyze large volumes of data quickly and efficiently.</li></ol>\n<p><strong><span style=\"font-size: 16px;\" data-redactor-style-cache=\"font-size: 16px;\" data-redactor-span=\"true\">Spark:</span></strong></p>\n<ul><li>Spark is a tool that helps to process and analyze large amounts of data quickly and efficiently. Think of it as a super-fast calculator that can do complex calculations on large sets of data in a very short amount of time.</li><li><span style=\"font-family: inherit; font-size: 0.875rem;\" data-redactor-span=\"true\" data-redactor-style-cache=\"font-family: inherit; font-size: 0.875rem;\">For example, let's say you have a large amount of customer data that you want to analyze to find out what products your customers are most interested in. Spark can help you process this data quickly and extract the insights you need to make informed business decisions.</span></li><li><span style=\"font-family: inherit; font-size: 0.875rem;\" data-redactor-span=\"true\" data-redactor-style-cache=\"font-family: inherit; font-size: 0.875rem;\">After data has been transformed and processed using Spark, it can be stored in a data warehouse or other storage system, where it can be analyzed further using business intelligence tools like Tableau or Power BI. Spark can also be used for machine learning, allowing businesses to build models that can be used to make predictions and optimize operations.</span></li></ul>\n<p><strong><span style=\"font-size: 16px;\" data-redactor-style-cache=\"font-size: 16px;\" data-redactor-span=\"true\">﻿Databricks:</span></strong></p>\n<p>﻿﻿While Spark is a powerful distributed computing engine designed for processing and analyzing large amounts of data quickly and efficiently, Databricks provides a range of additional features and capabilities that make it easier to use and more powerful.</p>\n<p>Some key features of Databricks include:</p>\n<ol><li>Unified Workspace: Databricks provides a unified workspace for data engineers, data scientists, and business analysts to collaborate and work with data using Spark.</li><li>Notebooks: Databricks provides a notebook interface that allows users to write and execute code in a collaborative environment. Notebooks can be used for data exploration, data cleaning, and machine learning tasks.</li><li>Machine Learning Capabilities: Databricks provides machine learning libraries and tools that can be used to build and deploy machine learning models.</li></ol>\n<p><strong><span data-redactor-style-cache=\"font-size: 16px;\" data-redactor-span=\"true\" style=\"font-size: 16px;\">Data Modeling:</span></strong>﻿</p><ul><li>Think of it like building a blueprint for a house. Before construction can begin, an architect must create a detailed plan that outlines the different rooms, hallways, and features of the house. Similarly, in data modeling, an analyst or developer must create a blueprint for a database that shows how different pieces of data relate to one another and how they can be organized.</li><li>Data modeling involves identifying the different entities, or objects, that will be stored in the database and defining the relationships between them. For example, if building a database for an online retailer, the entities might include customers, orders, and products, with relationships defined between them. The model will also specify the data types and constraints for each attribute, such as whether a field is required or optional.</li><li>The resulting data model serves as a guide for developers as they build the actual database. It can help ensure that the data is organized in a logical and efficient manner, and that it is consistent with the business requirements. It can also make it easier to modify the database later on if needed.</li><li>In summary, data modeling is the process of creating a blueprint for a database that outlines how different pieces of data relate to one another. This helps developers create a well-organized and efficient database that aligns with business requirements.</li></ul><p><br><br></p>"
    prescreenurl: '<p>Pipeline:&nbsp;﻿Data Engineer:&nbsp;﻿﻿<a href="https://app.willotalent.com/invite/onIV2D/">https://app.willotalent.com/in...</a>﻿</p>'
    payrateguidance: '<p>Level 2 - $65-$70</p><p>Level 3 - $70-$75</p>'
    submissionnotes: '<p>95-125</p>'
    skillchallengequestions: '<p><a href="https://gentissolutions.sharepoint.com/:w:/s/recruiters/Ea57z4ofd4VKjax6OMkByfcBxQGq59qa20LXSPHNmcq8Qw?e=JWLAnx"><a href="https://gentissolutions.sharepoint.com/:w:/s/recruiters/Ea57z4ofd4VKjax6OMkByfcBxQGq59qa20LXSPHNmcq8Qw?e=ZanKL6">Azure Data Engineer.docx</a>﻿</a>﻿</p>'
taxonomy:
    category:
        - jobs
        - ''
    tag:
        - ''
---

<p>We are seeking a skilled Senior Data Engineer with a minimum of 8 years of experience and extensive knowledge of data principles, patterns, processes, and practices. The ideal candidate will have experience defining evolutionary data solutions and underlying technologies, as well as strong analytical skills with attention to detail and accuracy. You will be responsible for analyzing and organizing raw data, building data systems and pipelines, evaluating business needs and objectives, interpreting trends and patterns, and conducting complex data analysis and reporting on results.</p>
<p><strong>Key Qualifications:</strong></p><ul><li>8+ years of experience as a Data Engineer</li><li>4+ years of experience with Azure Data Platform stack: Azure Data Lake, Azure Synapse, Data Factory, and Data bricks<ul></ul></li><li>4+ years of experience with Python, Spark, and SQL</li><li>Experience with SSAS Tabular models, Power BI, Dataflows, and DAX</li><li>Any experience with streaming technologies like Kafka, IBM MQ, and EventHub</li><li>Strong experience with SQL database design, data modeling</li><li>Basic understanding of network and data security architecture</li><li>Knowledge in a minimum of two of the following technical disciplines: data warehousing, data management, analytics development, data science, application programming interfaces (APIs), data integration, cloud, servers and storage, and database management</li><li>Experience building cost-effective and performance-driven solutions using elastic architectures in Microsoft Azure leveraging Cosmos, Azure Data Factory, Azure Synapse, and Databricks Platforms.</li><li>Experience with SQL and NoSQL applications on Big Data Platforms</li></ul>
<p><strong>Desired Previous Experience/Education:</strong><br></p>
<ul><li>Any experience with operational data science, machine learning, or artificial intelligence solutions</li><li>Any experience with data science solutions or platforms</li><li>Data engineering certifications is a plus</li><li>Any experience with a variety of SQL, NoSQL, and Big Data Platforms</li><li>Any experience building solutions using elastic architectures</li><li>Any experience with Infrastructure as Code / Terraform</li></ul>
<p><strong>Key Responsibilities:</strong></p>
<ul><li>Drive digital innovation by leveraging innovative new technologies and approaches to renovate, extend, and transform the existing core data assets, including SQL-based, NoSQL-based, and Cloud-based data platforms</li><li>Define high-level migration plans to address the gaps between the current and future state</li><li>Present opportunities with cost/benefit analysis to leadership to shape sound architectural decisions</li><li>Lead the analysis of the technology environment to detect critical deficiencies and recommend solutions for improvement</li><li>Interpreting data, analyzing results using statistical techniques</li><li>Developing and implementing data analyses, data collection systems, and other strategies that optimize statistical efficiency and quality</li><li>Acquiring data from primary or secondary data sources and maintaining databases</li><li>Promote the reuse of data assets, including the management of the data catalog for reference</li></ul>